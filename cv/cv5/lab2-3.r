#' ---
#' title: Lab2.3
#' author: Ema Richnakova
#' ---

# Classification problems 2

# TEST?
## concept of contingency table and statistics
#
## what is ROC and auROC (see if there is better probability cut-off (compared to default 50%))
#
## what is cut-off?
#

# Load players_22.csv data into R by completing the missing code.
library(tidyverse)
library(magrittr) # Pipe implementation for R
library(data.table) # Implements %like%

#setwd("./cv/cv5") # Set the correct path to players_22.csv
data <- read_csv("players_22.csv", col_names = TRUE, num_threads = 4) # Read data into R
data %<>% mutate(
  role = case_when(
    player_positions %like% "ST|RW|LW|CF|CAM" ~ "offense",
    player_positions %like% "LB|LWB|RB|RWB|RB|CDM" ~ "defense",
    player_positions %like% "GK" ~ "goalkeeper",
    .default = "midfielder"
  )
)
data %<>%
    mutate(role_oneHot = ifelse(role == "offense", 1, 0)) %<>% # 1 = offensive player, 0 = every other player
    select(short_name,long_name,overall,potential,preferred_foot,skill_dribbling,role,role_oneHot)

### review model parameters
# generate logistic regression model
model <- glm(data$role_oneHot ~ data$skill_dribbling + data$preferred_foot, family = binomial)
summary(model)

### model parameters and model performance
# model summary do not tell much about model error itself
# The performance of a logistic regression model is calculated by cross-tabulating the real class labels with predicted class labels.

### task 1
# Classification system, where “offensive” is the positive class (1) and “defensive/midfielder”
# is the negative class (0). Extract one-hot-encoded classes from the model object or the original 
# tibble data (hint: these data exist in both). You should get a numeric  vector of 0s and 1s as shown below.
real.classes <- data$role_oneHot
head(model$y)

### task 2
# Extract class probabilities from the model object. Use a 50% cut-off value for player 
# classification. Save the resulting vector as predicted.classes.
library(dplyr)
predicted.class.probabilities <- predict(model, type="response")
predicted.classes <-ifelse(predicted.class.probabilities > 0.5, 1, 0)
map_df(list(Real.Classes = real.classes,
            Predicted.Probabilities = predicted.class.probabilities,
            Predicted.CLasses = predicted.classes), head)

### task 3
# Cross-tabulate real and predicted classes. Use confusionMatrix() from the caret package.
# confusionMatrix() accepts predicted.classes and real.classes only as factors.
library(caret)
confusion.matrix <- caret::confusionMatrix(as_factor(predicted.classes), as_factor(real.classes), positive = "1")
confusion.matrix

### task 4
# Inspect your readout. Which of the above statistics have you computed in the last task of EIA Lab 2.2?
# postive class ?

### task 5
# Revisit the contingency matrix from Task 3. Fill in a R code that calculates accuracy using the following formula.
# \(Accuracy = \dfrac{TP + TN}{P + N}\)
ct <- confusion.matrix$table # contingencna tabulka
TP <- ct[2,2]
FP <- ct[2,1]
TN <- ct[1,1]
FN <- ct[1,2]
P <- TP+FN
N <- FP+TN

accuracy <- (TP+TN)/(P+N)
accuracy

### task 6
# You can now calculate the proportion of classes with positive and negative 
# labels to see how much is your dataset imbalanced. Is it?
proportion.of.1 <- P/(P+N) # TODO ?
proportion.of.0 <- N/(P+N) # TODO ?
paste("Class proportions are: ", proportion.of.1, proportion.of.0, "for 1 and 0, respectively." )

### task 7
# Calculate precision after retrieving counts from the confusion matrix.
# \(Precision = \dfrac{TP}{FP+TP}\)
precision <- TP/(FP+TP)
precision # quantifies the proportion of correct “positive” predictions made by the model.
NPV <- TN /(TN+FN)
NPV

### task 8
# You have already calculated precision, so let’s calculate recall this time.
# Use information from the contingency table generated by caret.
# \(Recall = \dfrac{TP}{P}\)
recall <- TP/P
recall # measures the proportion of positive class samples the model correctly identifies.

# F1 score
# blends precision and recall using their harmonic mean. Maximizing for the F1 
# score implies simultaneously maximizing for both precision and recall.
# \(F1 = 2* \dfrac{Precision * Recall}{Precision + Recall}\)

### task 9
# The caret-implemented confusionMatrix() function calculates a lot of statistics,
# but not F1 score. Your task is to calculate F1 score using the data provided by caret.
f1score <- 2*(precision*recall)/(precision+recall)
f1score

# As a general guideline, an F1 score of 0.7 or higher is often considered good.
specificity <- TN / (TN+FD)
specificity # o pozitivnych nam to povie
# o negativnych